<div class="container">
  <div class="jumbotron p-3 p-md-5 text-white rounded bg-dark">

</div>

<main role="main" class="container" style="background-color: #d6ba83; padding-left: 3%; padding-top: 2%;">
  <div class="row">
    <div class="col-md-8 blog-main">
      <h3 class="pb-3 mb-4 font-italic border-bottom">
      <strong>Final Year Project</strong>
      </h3>

      <div class="blog-post">
        <p class="blog-post-meta">June 15, 2020 by <a href="#">Omer Hameed,Shahira Malik,Sana Mohib</a></p>
        <h3><strong>Abstract:</strong></h3>
        <p>We present the Sketchy database, the first large-scale collection of sketch-photo pairs</p>
        <p>We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. <a href="#">The Sketchy database gives us finegrained associations between particular photos and sketches </a>We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both handcrafted features as well as deep features trained for sketch or photo classification</p>
        <p>Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.The goal of Sketch-based image retrieval is to allow non-artist users to draw visual content (usually objects) and then find matching examples in an image collection.Sketch-based image retrieval is an alternative or a complement to widely used language-based image querying (e.g. Google image search).</p>
        <p>In the computer graphics community, sketch-based image retrieval has been used to drive image synthesis approaches such as Sketch2Photo  <em>]. Sketch-based image retrieval has been studied for nearly 25 years [Kato et al. 1992], but is especially relevant as touch and pen-based devices have proliferated over the last few years</em> Typical approaches propose a hand-designed feature, usually focused on edges or gradients, which is somewhat invariant across domains.</p>
        
        <li>Sketch-based image retrieval</li>
        <li>Deep learning, Siamese network</li> 
        <li> Triplet network</li>
        <li>Image synthesis</li>
        <h3><strong>Introduction:</strong></h3>
        <p>. The primary reason sketch-to-image comparison is hard is that humans are not faithful artists.</p>
        <p>We tend to draw only salient object structures and we tend to draw them poorly. </p>
        <p>In fact, this strategy is consistent with common sketch-based image retrieval benchmarks [Hu and Collomosse 2013] where retrieval results are correct as long as they are the same category.</p>
        <h3><strong>Aims &Objectives:</strong></h3>
        <ul>
          <li>Our primary objective is to stimulate an internet based utility which can identify the sketches and produce the results closest to the end results of provided pictures in the form of free hand sketches of drawings. </li>
          <li>Working in this field specifically specialized in the extraction of the consultant and sharing features for sketches and herbal pictures.</li>
          <li>The recovery framework utilizing portrayals can be viable and basic in our everyday life, for example, Medical analysis, advanced library, web crawlers, wrongdoing anticipation</li>
        </ul>
        <p><strong>We make the following contributions:</strong></p>
        <ol>
          <li>• We demonstrate the first deep learning approach to sketchbased image retrieval. We learn a common feature space for sketches and photos which enables not only sketch-based image retrieval but also image-based sketch retrieval or sketch-to-sketch and photo-to-photo comparisons. </li>
          <li>• We show that our learned representation leads to significantly better fine-grained sketch retrieval than existing methods and even outperforms a hypothetical ideal “retrieval by categorization” method.</li>
          <li>• We develop a crowd data collection technique to collect representative (often bad!) object sketches prompted by but not traced from particular photos. The Sketchy database contains 75,471 sketches of 12,500 objects spanning 125 categories.</li>
        </ol>
        <p><strong> Related Work:</strong></p>
      </div><!-- /.blog-post -->

      <div class="blog-post">
        <h2 class="blog-post-title"></h2>
        <p class="blog-post-meta">December 23, 2013 by <a href="#">Jacob</a></p>

        <p><strong>Deep learning for cross-domain embedding:</strong></p>
        <blockquote>
          <p>Our technical approach is similar to recent cross-domain embedding methods that train deep networks to learn a common feature space for Sketches and 3D models [Wang et al. 2015], ground and aerial photographs [Lin et al. 2015], Iconic and in-the-wild product photos [Bell and Bala 2015], and Images and 3D models [Li et al. 2015]. We experiment with the tools used in these works such as Siamese networks trained with contrastive loss [Chopra et al. 2005; Hadsell et al. 2006], but find that triplet or ranking loss [Wang et al. 2014] performs better. Our best performing method combines the Triplet loss with a classification loss similar to Bell and Bala’s [2015] combination of Siamese and classification losses.</p>
        </blockquote>
      </div><!-- /.blog-post -->

      <div class="blog-post">
        <h2 class="blog-post-title"></h2>
        <p class="blog-post-meta">December 14, 2013 by <a href="#"></a></p>

        <p><strong>Photograph Selection:</strong></p>
        <ul>
          <li>We cannot expect novice artists to draw photographs of horse eyes and crocodile teeth in meaningful ways, yet these extreme photographs are prevalent in Internet-scale image data sets</li>
          <li>In order to select appropriate photographs for our data set, we first eliminate all photographs that do not have exactly one bounding box annotation. </li>
          <li> Overall, we review a total of 69,495 photographs and deem 24,819 as “sketchable”. This process results in a median of 147 sketchable photographs per category. Categories with fewer than 100 sketchable photographs are not included in our data set.</li>
        </ul>
        <p><strong>Sketch Collection:</strong></p>
        <p> The creation of sketch-photo pairs is the most critical challenge of creating the Sketchy database. There are two broad strategies – prompt the creation of sketches from particular photos [Eitz et al. 2010] or have people associate existing sketches to photos, e.g. by ranking a list of potentially matching photos [Eitz et al. 2011a].We choose the first strategy because it is better able to create finegrained, instance-level associations. With the second strategy, there may not exist a photo that is particularly similar to a sketch.</p>
      </div><!-- /.blog-post -->

      <nav class="blog-pagination">
        <a class="btn btn-outline-primary" href="#">Older</a>
        <a class="btn btn-outline-secondary disabled" href="#">Newer</a>
      </nav>

    </div><!-- /.blog-main -->

    <aside class="col-md-4 blog-sidebar">
      <div class="p-3 mb-3 bg-light rounded">
        <h4 class="font-italic">About</h4>
        <p class="mb-0"><em>Pattern recognition</em> is the process of recognizing patterns by using machine learning algorithm. Pattern recognition can be defined as the classification of data based on knowledge already gained or on statistical information extracted from patterns and/or their representation.</p>
      </div>

      <div class="p-3">
        <h4 class="font-italic">Archives</h4>
        <ol class="list-unstyled mb-0">
          <li><a href="#">March 2014</a></li>
          <li><a href="#">February 2014</a></li>
          <li><a href="#">January 2014</a></li>
          <li><a href="#">December 2013</a></li>
          <li><a href="#">November 2013</a></li>
          <li><a href="#">October 2013</a></li>
          <li><a href="#">September 2013</a></li>
          <li><a href="#">August 2013</a></li>
          <li><a href="#">July 2013</a></li>
          <li><a href="#">June 2013</a></li>
          <li><a href="#">May 2013</a></li>
          <li><a href="#">April 2013</a></li>
        </ol>
      </div>

      <div class="p-3">
        <h4 class="font-italic">Elsewhere</h4>
        <ol class="list-unstyled">
          <li><a href="#">GitHub</a></li>
          <li><a href="#">Twitter</a></li>
          <li><a href="#">Facebook</a></li>
        </ol>
      </div>
    </aside><!-- /.blog-sidebar -->

  </div><!-- /.row -->

</main><!-- /.container -->